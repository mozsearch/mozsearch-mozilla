#!/usr/bin/env bash

set -x # Show commands
set -eu # Errors/undefined vars are fatal
set -o pipefail # Check all commands in a pipeline

date

echo Downloading Gecko
pushd $INDEX_ROOT
$CONFIG_REPO/shared/fetch-gecko-tarball.sh gecko-dev $PWD
popd

date

echo Downloading Gecko blame
pushd $INDEX_ROOT
$CONFIG_REPO/shared/fetch-gecko-tarball.sh gecko-blame $PWD
popd

date

echo Downloading git to hg map
$CONFIG_REPO/shared/fetch-hg-map.sh

date

echo Updating git
pushd $GIT_ROOT
# The repo may have been uploaded while checked out to some non-master
# revision (to match the hg revision used in taskcluster)
git checkout master
git pull
popd

date

function move_file {
    mkdir -p "$(dirname $2)"
    mv "$1" "$2"
}

# Check that all the files (provided as arguments) are the same, after normalizing
# Windows line endings and paths to UNIX. At least one argument must be provided.
# If all the files match, the name of the first one is echo'd, otherwise the empty
# string is echo'd.
function check_all_same {
    if [ $# -eq 0 ]; then
        # At least one arg must be provided
        return 1;
    fi
    FIRSTFILE=$1; shift;
    # Normalize to UNIX in-place. The z: absolute path comes from the taskcluster
    # working directory for Windows.
    dos2unix --quiet "$FIRSTFILE"
    sed --in-place -e "s#z:/task_[0-9]*/#/builds/worker/workspace/#g" "$FIRSTFILE"
    while [ $# -gt 0 ]; do
        NEXTFILE=$1; shift;
        dos2unix --quiet "$NEXTFILE"
        sed --in-place -e "s#z:/task_[0-9]*/#/builds/worker/workspace/#g" "$NEXTFILE"
        cmp --quiet "$FIRSTFILE" "$NEXTFILE"
        if [ $? -ne 0 ]; then
            # Files aren't the same, echo empty string
            echo ""
            return 0;
        fi
    done
    # All files are the same, echo any one of them
    echo "$FIRSTFILE"
    return 0;
}

# Edit this to e.g. try.revision.ee64db93dcc149da9313460317257b8c42eec5b2 or whatever
# to test other revisions. If you do this, also override INDEXED_GIT_REV below to some sane
# git-equivalent of the revision because most likely it won't find anything in the mapfile.
# This defaults to the most recent searchfox indexing run for mozilla-central.
REVISION_TREE=mozilla-central       # replace with e.g. 'try' for testing
REVISION_ID=latest                  # replace with e.g. 'revision.ee64db93dcc149da9313460317257b8c42eec5b2' for testing
REVISION="${REVISION_TREE}.${REVISION_ID}"

CURL="curl -SsfL --compressed"

pushd $INDEX_ROOT
${CURL} https://index.taskcluster.net/v1/task/gecko.v2.$REVISION.firefox.linux64-searchfox-debug/artifacts/public/build/target.json > target.json
INDEXED_HG_REV=$(python $MOZSEARCH_PATH/scripts/read-json.py target.json moz_source_stamp)
INDEXED_GIT_REV=$(awk -v REV=$INDEXED_HG_REV '$2 ~ REV { print $1 }' "${WORKING}/git_hg.map")

# If INDEXED_GIT_REV gets set, that means the gecko-dev repo includes the code for which
# the taskcluster indexing job ran. So we can download the result of that indexing
# job and use it. Otherwise, it means gecko-dev is lagging behind the canonical hg
# repo, and we don't have the source corresponding to the indexing run on taskcluster.
# In that case we don't use the analysis data at all, and offer plaintext search and
# blame on the latest gecko-dev that we do have. In practice this scenario rarely
# gets hit.

if [ -n "$INDEXED_GIT_REV" ]; then
    pushd $GIT_ROOT
    # Since we're using a prebuilt index at a particular rev, sync our local repo to the same rev
    git checkout $INDEXED_GIT_REV
    popd

    rm -f generated-files.list analysis-files.list analysis-dirs.list

    date

    $CONFIG_REPO/shared/fetch-tc-artifacts.sh $REVISION_TREE $INDEXED_HG_REV

    # And now process the downloads
    for PLATFORM in linux64 macosx64 win64 android-armv7; do
        date
        echo "Processing platform $PLATFORM"

        if [ ! -f "$PLATFORM.mozsearch-index.zip" ]; then
            # This platform didn't have analysis data, so let's skip it
            continue
        fi

        # Unpack the C++ analysis into a platform-specific folder
        mkdir -p analysis-$PLATFORM
        unzip -q $PLATFORM.mozsearch-index.zip -d analysis-$PLATFORM

        # These all get unpacked into the objdir directly because the files
        # are already in platform-specific subfolders inside the zipfile, so there won't be any collisions. The
        # rust-indexer.rs tool will take care of combining all the analysis files correctly.
        unzip -q $PLATFORM.mozsearch-rust.zip -d objdir

        # Unpack generated sources tarballs into platform-specific folder
        mkdir -p generated-$PLATFORM
        tar -x -z -C generated-$PLATFORM -f $PLATFORM.generated-files.tar.gz

        # Process the dist/include manifest and normalize away the taskcluster paths
        dos2unix --quiet --force $PLATFORM.distinclude.map  # need --force because of \x1f column separator chars in the file
        MAPVERSION=$(head -n 1 $PLATFORM.distinclude.map)
        if [ "$MAPVERSION" != "5" ]; then
            echo "WARNING: $PLATFORM.distinclude.map had unexpected version [$MAPVERSION]; check for changes in python/mozbuild/mozpack/manifests.py."
        fi
        sed --in-place -e "s#/builds/worker/workspace/build/src/##g" $PLATFORM.distinclude.map
        sed --in-place -e "s#z:/task_[0-9]*/build/src/##g" $PLATFORM.distinclude.map

        date

        # Special cases - buildid.h and mozilla-config.h show up in two places in a regular
        # objdir and the analysis tarball will correspondingly also have two instances of
        # the analysis file. The generated-files tarball only has one copy, so we manually
        # make the other copy to make things match up. The best would be if the gecko build
        # system didn't actually produce two copies of these files.
        cp generated-$PLATFORM/buildid.h        generated-$PLATFORM/dist/include/buildid.h
        cp generated-$PLATFORM/mozilla-config.h generated-$PLATFORM/dist/include/mozilla-config.h

        # We get analysis data for generated files, some of which aren't included
        # in the generated-files tarball that we get from taskcluster. Most of these
        # missing files are dummy unified build files that we don't care about, and
        # we can just delete those.
        # If there are other such cases, then we'll get zero-byte files generated by
        # output-file.rs since it won't be able to find the source file corresponding to
        # the analysis file. In those cases we should ensure the generated file is
        # included in the target.generated-files.tar.gz tarball that comes out of the
        # taskcluster indexing job. Bug 1440879 can be used as a guide.
        pushd analysis-$PLATFORM/__GENERATED__
        set +x  # Turn off echoing of commands and output only relevant things to avoid bloating logfiles
        find . -type f -name "Unified*" |
        while read GENERATED_ANALYSIS; do
            if [ ! -f "$INDEX_ROOT/generated-$PLATFORM/$GENERATED_ANALYSIS" ]; then
                echo "Remove unified compilation generated-file $GENERATED_ANALYSIS"
                rm "$GENERATED_ANALYSIS"
            fi
        done
        set -x
        popd

        date

        # On Windows, analysis for headers ends up in __GENERATED__/dist/include/...
        # instead of the source version of the file. This happens because Windows doesn't
        # support symlinks, and so during the build, headers are copied rather than
        # symlinked into dist/include. When clang processes the source files, it therefore
        # can't "dereference" the symlinks (because they're not symlinks) to find the
        # original source file. This results in the misplaced analysis data. To handle
        # this, we use the distinclude mapfile produced by the taskcluster job to
        # squash the analysis data for such files back to the source file that it belongs
        # with. The "squash" is just appending the analysis data from the dist/include
        # version to the analysis data (if it exists) for the real source. The step
        # to merge the analyses across platforms later in this script will deduplicate
        # any redundant lines.
        # Note also that this hunk of code currently does nothing on Linux/Mac, since
        # don't suffer from this problem. The code is run anyway for completeness.
        pushd analysis-$PLATFORM/__GENERATED__/dist/include
        set +x  # Turn off echoing of commands and output only relevant things to avoid bloating logfiles
        find . -type f |
        while read GENERATED_ANALYSIS; do
            if [ ! -f "$INDEX_ROOT/generated-$PLATFORM/dist/include/$GENERATED_ANALYSIS" ]; then
                # Found analysis file in __GENERATED__/dist/include for which there is
                # no corresponding generated source. Let's check the mapfile to see if
                # the source is elsewhere. The awk command searches the mapfile, assuming
                # columns are separated by the \x1f character, looking for the first match
                # where the second column (with a "./" prefixed) is the same as
                # $GENERATED_ANALYSIS. If a match is found, it emits the third column, which
                # is the path of the source tree relative to the gecko-dev root (because of
                # the normalization step after downloading the file).
                REAL_SOURCE=$(awk -F '\x1f' -v KEY="$GENERATED_ANALYSIS" 'KEY == "./" $2 { print $3; exit }' ../../../../$PLATFORM.distinclude.map)
                if [ -f "$INDEX_ROOT/gecko-dev/$REAL_SOURCE" ]; then
                    # Found the real source file this analysis data is for, so let's squash it over
                    ANALYSIS_FOR_SOURCE="$INDEX_ROOT/analysis-$PLATFORM/$REAL_SOURCE"
                    echo "Squashing analysis for __GENERATED__/dist/include/$GENERATED_ANALYSIS into analysis-$PLATFORM/$REAL_SOURCE"
                    mkdir -p $(dirname "$ANALYSIS_FOR_SOURCE")
                    cat "$GENERATED_ANALYSIS" >> "$ANALYSIS_FOR_SOURCE"
                    rm "$GENERATED_ANALYSIS"
                else
                    echo "Real source [$REAL_SOURCE] for dist/include/$GENERATED_ANALYSIS was not found or was not a file"
                fi
            fi
        done
        set -x
        popd

        date

        # Also drop any directories that got emptied as a result
        pushd analysis-$PLATFORM/__GENERATED__
        find . -depth -type d -empty -delete
        popd

        pushd generated-$PLATFORM
        find . -type f >> ../generated-files.list
        popd

        # List all the analysis files we have left. We will merge these across platforms
        # after this per-platform loop is complete. Make sure to skip over the __GENERATED__
        # directory
        pushd analysis-$PLATFORM
        find . -not \( -name __GENERATED__ -prune \) -type d >> ../analysis-dirs.list
        find . -not \( -name __GENERATED__ -prune \) -type f >> ../analysis-files.list
        popd

    done    # end PLATFORM loop

    date

    # Special case: xptdata.cpp is a giant file and is different for each platform, but
    # the differences are not particularly relevant so let's just keep the Linux one.
    for PLATFORM in macosx64 win64 android-armv7; do
        rm -f generated-${PLATFORM}/xpcom/reflect/xptinfo/xptdata.cpp
        rm -f analysis-${PLATFORM}/__GENERATED__/xpcom/reflect/xptinfo/xptdata.cpp
    done

    # For each generated file, if all platforms generated the same thing (or didn't
    # generate the file at all due to being a platform-specific feature), copy it to
    # the merged objdir.
    set +x  # Turn off echoing of commands and output only relevant things to avoid bloating logfiles
    sort generated-files.list | uniq |
    while read GENERATED_FILE; do
        ALL_SAME_AS=$(check_all_same generated-*/$GENERATED_FILE)
        if [ "$ALL_SAME_AS" != "" ]; then
            echo "Generated file $GENERATED_FILE was identical across platforms where it was created"
            move_file "$ALL_SAME_AS" "objdir/$GENERATED_FILE"
            # Also merge the analyses files
            MERGED_ANALYSIS="analysis/__GENERATED__/$GENERATED_FILE"
            mkdir -p "$(dirname $MERGED_ANALYSIS)"
            RUST_LOG=info $MOZSEARCH_PATH/tools/target/release/merge-analyses analysis-*/__GENERATED__/$GENERATED_FILE > $MERGED_ANALYSIS
            continue;
        fi

        # The generated file was not the same across all platforms, so
        # put the different versions in __$PLATFORM__ subfolders.
        for PLATFORM in linux64 macosx64 win64 android-armv7; do
            if [ ! -f "generated-$PLATFORM/$GENERATED_FILE" ]; then
                continue;
            fi
            echo "Taking generated file $GENERATED_FILE from $PLATFORM"
            move_file "generated-$PLATFORM/$GENERATED_FILE" "objdir/__${PLATFORM}__/$GENERATED_FILE"
            if [ -f "analysis-$PLATFORM/__GENERATED__/$GENERATED_FILE" ]; then
                # Move the analysis file as well
                move_file "analysis-$PLATFORM/__GENERATED__/$GENERATED_FILE" "analysis/__GENERATED__/__${PLATFORM}__/$GENERATED_FILE"
            fi
        done
    done
    set -x

    # Throw away any leftover per-platform generated files, and the analysis data
    # for generated files. The above loop should have extracted all the useful
    # information from these folders into the objdir/ and analysis/__GENERATED__/
    # folders.
    rm -rf generated-*
    rm -rf analysis-*/__GENERATED__

    date

    # Finally, merge the analysis files for the non-generated source files. All the files
    # are going to be listed in the analysis-files.list, possibly duplicated across
    # platforms, so we deduplicate the filenames and merge each filename across platforms.
    set +x  # Turn off echoing of commands and output only relevant things to avoid bloating logfiles
    sort analysis-dirs.list | uniq |
    while read ANALYSIS_DIR; do
        mkdir -p "analysis/$ANALYSIS_DIR"
    done
    sort analysis-files.list | uniq |
    while read ANALYSIS_FILE; do
        echo "Merging analyses for $ANALYSIS_FILE"
        RUST_LOG=info $MOZSEARCH_PATH/tools/target/release/merge-analyses analysis-*/$ANALYSIS_FILE > analysis/$ANALYSIS_FILE
    done
    set -x

    # Free up disk space, we don't need these per-platform analysis files any more.
    rm -rf analysis-*

else
    echo "WARNING: Unable to find git equivalent for hg rev $INDEXED_HG_REV; omitting Rust/C++ analysis..."
fi
popd

date

# Generate the blame information after checking out the GIT_ROOT to appropriate
# revision above, so that the blame repo's head matches the git repo's head.
echo "Generating blame information for central..."
python $MOZSEARCH_PATH/blame/transform-repo.py $GIT_ROOT $BLAME_ROOT $WORKING/git_hg.map

# Also generate blame for some other branches, because the gecko-blame repo is
# shared by those branches. We do this here instead of in e.g. ../mozilla-beta/setup
# because it's best to have only one indexer instance responsible for updating and
# pushing the tarball to S3, to avoid accidental clobbers. It's not a great situation
# architecturally, but it's better for performance.
for BRANCH in beta release esr60; do
    echo "Updating gecko-dev branch $BRANCH to latest from upstream..."
    pushd $GIT_ROOT
    git branch -f $BRANCH origin/$BRANCH
    popd
    echo "Generating blame information for $BRANCH..."
    BLAME_REF="refs/heads/$BRANCH" python $MOZSEARCH_PATH/blame/transform-repo.py $GIT_ROOT $BLAME_ROOT $WORKING/git_hg.map
done

# Point the blame repo's HEAD to the commit matching what we have in in the src repo. Note
# that we use `git reset --soft` because we don't need anything in the repo's working dir.
if [ -n "$INDEXED_GIT_REV" ]; then
    pushd $BLAME_ROOT
    BLAME_REV=$(git log -1 --grep=$INDEXED_GIT_REV --pretty=format:%H)
    if [ -z "$BLAME_REV" ]; then
        echo "Unable to find blame rev for $INDEXED_GIT_REV"
        exit 1;
    fi
    git reset --soft $BLAME_REV
    popd
fi

date
